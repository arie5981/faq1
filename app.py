import streamlit as st
import os
import re
import unicodedata
import streamlit as st
import copy
from dataclasses import dataclass
from typing import List, Optional
from rapidfuzz import fuzz
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import openai

# ========== ×”×’×“×¨×•×ª ==========
st.set_page_config(page_title="×¢×•×–×¨ ××ª×¨ ×ž×™×™×¦×’×™×", layout="wide")
st.title("ðŸŸ¦ ×¢×•×–×¨ ××ª×¨ ×ž×™×™×¦×’×™× â€“ ×’×¨×¡×ª ×“×ž×• ××™× ×˜×¨× ×˜×™×ª")

# ×§×œ×˜ API key ×ž×¦×“ ×”×ž×©×ª×ž×©
# api_key = st.text_input("ðŸ”‘ ×”×›× ×¡ ×ž×¤×ª×— OpenAI:", type="password")

# ×˜×¢×™× ×ª ×”×ž×¤×ª×— ×ž×ª×•×š Streamlit Secrets
api_key = st.secrets["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = api_key

if not api_key:
    st.info("×”×›× ×¡ ×ž×¤×ª×— API ×›×“×™ ×œ×”×ª×—×™×œ.")
    st.stop()

openai.api_key = api_key
os.environ["OPENAI_API_KEY"] = api_key

# ========== ×”×¢×œ××ª ×§×•×‘×¥ FAQ ==========
st.subheader("ðŸ“„ ×”×¢×œ×” ×§×•×‘×¥ FAQ (×˜×§×¡×˜ ×‘×¤×•×¨×ž×˜ UTF-8):")
uploaded_file = st.file_uploader("×‘×—×¨ ×§×•×‘×¥ faq.txt", type=["txt"])

if not uploaded_file:
    st.warning("×™×© ×œ×”×¢×œ×•×ª ×§×•×‘×¥ FAQ ×›×“×™ ×œ×”×ž×©×™×š.")
    st.stop()

raw_faq = uploaded_file.read().decode("utf-8")

# ========== ×¤×•× ×§×¦×™×•×ª × ×•×¨×ž×œ×™×–×¦×™×” ==========
def normalize_he(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFC", s)
    s = re.sub(r"[\u200e\u200f]", "", s)
    s = re.sub(r"[^\w\s\u0590-\u05FF]", " ", s)
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s

# ========== ×ž×‘× ×” FAQ ==========
@dataclass
class FAQItem:
    question: str
    variants: List[str]
    answer: str

# ========== ×¤×™×¨×•×§ ×”-FAQ ==========
def parse_faq(text: str) -> List[FAQItem]:
    items = []
    blocks = re.split(r"(?=×©××œ×”\s*:)", text)
    for b in blocks:
        b = b.strip()
        if not b:
            continue

        q_match = re.search(r"×©××œ×”\s*:\s*(.+)", b)
        a_match = re.search(r"(?s)×ª×©×•×‘×”\s*:\s*(.+?)(?:\n×”×•×¨××”\s*:|\Z)", b)
        v_match = re.search(r"(?s)× ×™×¡×•×—×™× ×“×•×ž×™×\s*:\s*(.+?)(?:\n×ª×©×•×‘×”\s*:|\Z)", b)

        question = q_match.group(1).strip() if q_match else ""
        answer = a_match.group(1).strip() if a_match else ""
        variants = []

        if v_match:
            raw = v_match.group(1)
            variants = [s.strip(" -\t") for s in raw.split("\n") if s.strip()]

        items.append(FAQItem(question, variants, answer))

    return items

faq_items = parse_faq(raw_faq)
st.success(f"× ×˜×¢× ×• {len(faq_items)} ×©××œ×•×ª ×ž×”-FAQ")

# ========== Embeddings ==========
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

docs = []
for i, item in enumerate(faq_items):
    merged = " | ".join([item.question] + item.variants)
    docs.append(Document(page_content=merged, metadata={"idx": i}))

faq_store = FAISS.from_documents(docs, embeddings)

# ========== ×ž× ×•×¢ ×—×™×¤×•×© ==========
def search_faq(query: str) -> Optional[str]:
    nq = normalize_he(query)

    verbs = {
        "add": ["×”×•×¡×£", "×œ×”×•×¡×™×£", "×”×•×¡×¤×”", "×ž×•×¡×™×£", "×ž×•×¡×™×¤×™×", "×œ×¦×¨×£", "×¦×™×¨×•×£", "×¤×ª×™×—×”", "×¤×ª×™×—×ª", "×¨×™×©×•×", "×œ×”×™×¨×©×"],
        "delete": ["×ž×—×§", "×ž×—×™×§×”", "×œ×”×¡×™×¨", "×”×¡×¨", "×”×¡×¨×”", "×‘×™×˜×•×œ", "×œ×‘×˜×œ", "×¡×’×•×¨", "×œ×¡×’×•×¨"],
        "update": ["×¢×“×›×Ÿ", "×œ×¢×“×›×Ÿ", "×¢×“×›×•×Ÿ", "×©×™× ×•×™", "×œ×©× ×•×ª", "×¢×¨×™×›×”", "×œ×ª×§×Ÿ"]
    }

    intent = None
    for k, words in verbs.items():
        if any(w in nq for w in words):
            intent = k
            break

    scored = []
    for i, item in enumerate(faq_items):
        all_texts = [item.question] + item.variants

        for t in all_texts:
            score = fuzz.token_sort_ratio(nq, normalize_he(t))

            t_intent = None
            for k, words in verbs.items():
                if any(w in t for w in words):
                    t_intent = k
                    break

            if intent and t_intent and intent != t_intent:
                score -= 50
            if intent and t_intent and intent == t_intent:
                score += 25

            scored.append((score, i, t))

    scored.sort(reverse=True, key=lambda x: x[0])
    top = scored[:5]

    best_score = top[0][0]
    best_index = top[0][1]

    if best_score >= 55:
        return faq_items[best_index]

    hits = faq_store.similarity_search_with_score(nq, k=4)
    hits = sorted(hits, key=lambda x: x[1])

    if hits and hits[0][1] < 1.2:
        idx = hits[0][0].metadata["idx"]
        return faq_items[idx]

    return None

# ========== ×ž×ž×©×§ ×ž×©×ª×ž×© ==========
st.subheader("â“ ×©××œ ×©××œ×”")

query = st.text_input("×”×§×œ×“ ×©××œ×” ×›××Ÿ:")
submit = st.button("×©×œ×—")

if submit and query:
    result = search_faq(query)

    if not result:
        st.error("×œ× × ×ž×¦××” ×ª×©×•×‘×”. × ×¡×” ×œ× ×¡×— ××—×¨×ª.")
    else:
        st.success("âœ“ × ×ž×¦××” ×ª×©×•×‘×”")
        st.write(result.answer)
        st.caption(f"ðŸ”¹ ×©××œ×” ×ž×–×•×”×”: {result.question}")




